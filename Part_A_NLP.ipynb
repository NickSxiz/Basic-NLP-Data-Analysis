{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kltLwOjK6UN3",
        "outputId": "2514ebe9-94b7-456b-86ed-e5f915bdca9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.794\n",
            "Classification Report:\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "            Astrology       0.90      0.82      0.86        11\n",
            "          Attractions       0.89      0.67      0.76        12\n",
            "           Automotive       1.00      0.86      0.92         7\n",
            "               Beauty       0.70      0.86      0.78        22\n",
            "     Business&Finance       0.84      0.98      0.91        55\n",
            "              Culture       0.81      0.88      0.84        64\n",
            "            Education       1.00      1.00      1.00         7\n",
            " Family&Relationships       0.82      0.53      0.64        17\n",
            "           Food&Drink       0.62      0.83      0.71         6\n",
            "       Healthy Living       0.82      0.86      0.84        36\n",
            "          Home&Garden       0.86      0.75      0.80         8\n",
            "             Politics       1.00      0.47      0.64        15\n",
            "          Pop Culture       0.00      0.00      0.00         9\n",
            "Religion&Spirituality       1.00      0.50      0.67        10\n",
            "              Science       0.85      0.85      0.85        27\n",
            "     Sensitive Topics       0.56      0.79      0.65        57\n",
            "               Sports       0.87      0.90      0.89        69\n",
            "        Style&Fashion       0.94      0.65      0.77        23\n",
            "       Tech&Computing       0.96      0.83      0.89        30\n",
            "               Travel       0.67      0.57      0.62         7\n",
            "       Viral Articles       0.25      0.12      0.17         8\n",
            "\n",
            "             accuracy                           0.79       500\n",
            "            macro avg       0.78      0.70      0.72       500\n",
            "         weighted avg       0.80      0.79      0.78       500\n",
            "\n",
            "                                                   url  \\\n",
            "316  https://hwbox.gr/news/ram/38975-ballistix-cruc...   \n",
            "317  https://nextluxury.com/mens-style-and-fashion/...   \n",
            "520  https://www.protothema.gr/travelling/article/9...   \n",
            "505  https://www.madamefigaro.gr/celebrities/hot-ne...   \n",
            "698  https://www.vimaorthodoxias.gr/agio-oros/agios...   \n",
            "240  https://www.zougla.gr/market-news/article/ola-...   \n",
            "391  https://www.ediva.gr/14-idees-diakosmisis-me-p...   \n",
            "661  https://www.maxmag.gr/perivallon/oi-pygolampid...   \n",
            "62   https://perpetual.gr/watches/o-ntouein-tzonson...   \n",
            "510  https://www.iefimerida.gr/ellada/anebainei-the...   \n",
            "\n",
            "                                               content        predicted_label  \n",
            "316  Η εταιρία αποσπάται από τις  Crucial/Micron  κ...         Tech&Computing  \n",
            "317  Τα τατουάζ quote στο στήθος είναι ένα στήριγμα...                 Beauty  \n",
            "520  \\n\\nΠρώτος προορισμός είναι ηγια τους Τούρκους...       Business&Finance  \n",
            "505  H  Nicole Kidman  είναι πια μία από τις διασημ...       Sensitive Topics  \n",
            "698  «Μοναχισμός και Αγιότητα στο Βυζάντιο» – Άγιος...  Religion&Spirituality  \n",
            "240  Βιβλία, ταινίες, παιχνίδια, αξεσουάρ, περιφερε...         Tech&Computing  \n",
            "391  Η κουζίνα είναι ένας χώρος που δεν δίνουμε και...            Home&Garden  \n",
            "661  Θυμάμαι την πρώτη φορά που αντίκρυσα πυγολαμπί...                Science  \n",
            "62   Πριν από μερικές ημέρες ο διάσημος ηθοποιός Ντ...         Tech&Computing  \n",
            "510  Γενικά αίθριος θα είναι ο καιρός σήμερα και η ...                Science  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the training data\n",
        "train_data_path = 'train_data.csv'\n",
        "train_data = pd.read_csv(train_data_path)\n",
        "\n",
        "# Simple text cleaning function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)  # Remove punctuation and numbers\n",
        "    text = text.strip()  # Remove extra spaces\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "# Combine title and content into a single text feature\n",
        "train_data['combined_text'] = train_data['title'] + \" \" + train_data['content']\n",
        "train_data['cleaned_text'] = train_data['combined_text'].apply(clean_text)\n",
        "\n",
        "# Define a small sample of Greek stopwords for demonstration\n",
        "greek_stopwords = [\"ένα\",\"έναν\",\"ένας\",\"αι\",\"ακομα\",\"ακομη\",\"ακριβως\",\"αληθεια\",\"αληθινα\",\"αλλα\",\"αλλαχου\",\"αλλες\",\"αλλη\",\"αλλην\",\"αλλης\",\"αλλιως\",\"αλλιωτικα\",\"αλλο\",\"αλλοι\",\"αλλοιως\",\"αλλοιωτικα\",\"αλλον\",\"αλλος\",\"αλλοτε\",\"αλλου\",\"αλλους\",\"αλλων\",\"αμα\",\"αμεσα\",\"αμεσως\",\"αν\",\"ανα\",\"αναμεσα\",\"αναμεταξυ\",\"ανευ\",\"αντι\",\"αντιπερα\",\"αντις\",\"ανω\",\"ανωτερω\",\"αξαφνα\",\"απ\",\"απεναντι\",\"απο\",\"αποψε\",\"από\",\"αρα\",\"αραγε\",\"αργα\",\"αργοτερο\",\"αριστερα\",\"αρκετα\",\"αρχικα\",\"ας\",\"αυριο\",\"αυτα\",\"αυτες\",\"αυτεσ\",\"αυτη\",\"αυτην\",\"αυτης\",\"αυτο\",\"αυτοι\",\"αυτον\",\"αυτος\",\"αυτοσ\",\"αυτου\",\"αυτους\",\"αυτουσ\",\"αυτων\",\"αφοτου\",\"αφου\",\"αἱ\",\"αἳ\",\"αἵ\",\"αὐτόσ\",\"αὐτὸς\",\"αὖ\",\"α∆ιακοπα\",\"βεβαια\",\"βεβαιοτατα\",\"γάρ\",\"γα\",\"γα^\",\"γε\",\"γι\",\"για\",\"γοῦν\",\"γρηγορα\",\"γυρω\",\"γὰρ\",\"δ'\",\"δέ\",\"δή\",\"δαί\",\"δαίσ\",\"δαὶ\",\"δαὶς\",\"δε\",\"δεν\",\"δι\",\"δι'\",\"διά\",\"δια\",\"διὰ\",\"δὲ\",\"δὴ\",\"δ’\",\"εαν\",\"εαυτο\",\"εαυτον\",\"εαυτου\",\"εαυτους\",\"εαυτων\",\"εγκαιρα\",\"εγκαιρως\",\"εγω\",\"ειθε\",\"ειμαι\",\"ειμαστε\",\"ειναι\",\"εις\",\"εισαι\",\"εισαστε\",\"ειστε\",\"ειτε\",\"ειχα\",\"ειχαμε\",\"ειχαν\",\"ειχατε\",\"ειχε\",\"ειχες\",\"ει∆εμη\",\"εκ\",\"εκαστα\",\"εκαστες\",\"εκαστη\",\"εκαστην\",\"εκαστης\",\"εκαστο\",\"εκαστοι\",\"εκαστον\",\"εκαστος\",\"εκαστου\",\"εκαστους\",\"εκαστων\",\"εκει\",\"εκεινα\",\"εκεινες\",\"εκεινεσ\",\"εκεινη\",\"εκεινην\",\"εκεινης\",\"εκεινο\",\"εκεινοι\",\"εκεινον\",\"εκεινος\",\"εκεινοσ\",\"εκεινου\",\"εκεινους\",\"εκεινουσ\",\"εκεινων\",\"εκτος\",\"εμας\",\"εμεις\",\"εμενα\",\"εμπρος\",\"εν\",\"ενα\",\"εναν\",\"ενας\",\"ενος\",\"εντελως\",\"εντος\",\"εντωμεταξυ\",\"ενω\",\"ενός\",\"εξ\",\"εξαφνα\",\"εξης\",\"εξισου\",\"εξω\",\"επ\",\"επί\",\"επανω\",\"επειτα\",\"επει∆η\",\"επι\",\"επισης\",\"επομενως\",\"εσας\",\"εσεις\",\"εσενα\",\"εστω\",\"εσυ\",\"ετερα\",\"ετεραι\",\"ετερας\",\"ετερες\",\"ετερη\",\"ετερης\",\"ετερο\",\"ετεροι\",\"ετερον\",\"ετερος\",\"ετερου\",\"ετερους\",\"ετερων\",\"ετουτα\",\"ετουτες\",\"ετουτη\",\"ετουτην\",\"ετουτης\",\"ετουτο\",\"ετουτοι\",\"ετουτον\",\"ετουτος\",\"ετουτου\",\"ετουτους\",\"ετουτων\",\"ετσι\",\"ευγε\",\"ευθυς\",\"ευτυχως\",\"εφεξης\",\"εχει\",\"εχεις\",\"εχετε\",\"εχθες\",\"εχομε\",\"εχουμε\",\"εχουν\",\"εχτες\",\"εχω\",\"εως\",\"εἰ\",\"εἰμί\",\"εἰμὶ\",\"εἰς\",\"εἰσ\",\"εἴ\",\"εἴμι\",\"εἴτε\",\"ε∆ω\",\"η\",\"ημασταν\",\"ημαστε\",\"ημουν\",\"ησασταν\",\"ησαστε\",\"ησουν\",\"ηταν\",\"ητανε\",\"ητοι\",\"ηττον\",\"η∆η\",\"θα\",\"ι\",\"ιι\",\"ιιι\",\"ισαμε\",\"ισια\",\"ισως\",\"ισωσ\",\"ι∆ια\",\"ι∆ιαν\",\"ι∆ιας\",\"ι∆ιες\",\"ι∆ιο\",\"ι∆ιοι\",\"ι∆ιον\",\"ι∆ιος\",\"ι∆ιου\",\"ι∆ιους\",\"ι∆ιων\",\"ι∆ιως\",\"κ\",\"καί\",\"καίτοι\",\"καθ\",\"καθε\",\"καθεμια\",\"καθεμιας\",\"καθενα\",\"καθενας\",\"καθενος\",\"καθετι\",\"καθολου\",\"καθως\",\"και\",\"κακα\",\"κακως\",\"καλα\",\"καλως\",\"καμια\",\"καμιαν\",\"καμιας\",\"καμποσα\",\"καμποσες\",\"καμποση\",\"καμποσην\",\"καμποσης\",\"καμποσο\",\"καμποσοι\",\"καμποσον\",\"καμποσος\",\"καμποσου\",\"καμποσους\",\"καμποσων\",\"κανεις\",\"κανεν\",\"κανενα\",\"κανεναν\",\"κανενας\",\"κανενος\",\"καποια\",\"καποιαν\",\"καποιας\",\"καποιες\",\"καποιο\",\"καποιοι\",\"καποιον\",\"καποιος\",\"καποιου\",\"καποιους\",\"καποιων\",\"καποτε\",\"καπου\",\"καπως\",\"κατ\",\"κατά\",\"κατα\",\"κατι\",\"κατιτι\",\"κατοπιν\",\"κατω\",\"κατὰ\",\"καὶ\",\"κι\",\"κιολας\",\"κλπ\",\"κοντα\",\"κτλ\",\"κυριως\",\"κἀν\",\"κἂν\",\"λιγακι\",\"λιγο\",\"λιγωτερο\",\"λογω\",\"λοιπα\",\"λοιπον\",\"μέν\",\"μέσα\",\"μή\",\"μήτε\",\"μία\",\"μα\",\"μαζι\",\"μακαρι\",\"μακρυα\",\"μαλιστα\",\"μαλλον\",\"μας\",\"με\",\"μεθ\",\"μεθαυριο\",\"μειον\",\"μελει\",\"μελλεται\",\"μεμιας\",\"μεν\",\"μερικα\",\"μερικες\",\"μερικοι\",\"μερικους\",\"μερικων\",\"μεσα\",\"μετ\",\"μετά\",\"μετα\",\"μεταξυ\",\"μετὰ\",\"μεχρι\",\"μη\",\"μην\",\"μηπως\",\"μητε\",\"μη∆ε\",\"μιά\",\"μια\",\"μιαν\",\"μιας\",\"μολις\",\"μολονοτι\",\"μοναχα\",\"μονες\",\"μονη\",\"μονην\",\"μονης\",\"μονο\",\"μονοι\",\"μονομιας\",\"μονος\",\"μονου\",\"μονους\",\"μονων\",\"μου\",\"μπορει\",\"μπορουν\",\"μπραβο\",\"μπρος\",\"μἐν\",\"μὲν\",\"μὴ\",\"μὴν\",\"να\",\"ναι\",\"νωρις\",\"ξανα\",\"ξαφνικα\",\"ο\",\"οι\",\"ολα\",\"ολες\",\"ολη\",\"ολην\",\"ολης\",\"ολο\",\"ολογυρα\",\"ολοι\",\"ολον\",\"ολονεν\",\"ολος\",\"ολοτελα\",\"ολου\",\"ολους\",\"ολων\",\"ολως\",\"ολως∆ιολου\",\"ομως\",\"ομωσ\",\"οποια\",\"οποιαν\",\"οποιαν∆ηποτε\",\"οποιας\",\"οποιας∆ηποτε\",\"οποια∆ηποτε\",\"οποιες\",\"οποιες∆ηποτε\",\"οποιο\",\"οποιοι\",\"οποιον\",\"οποιον∆ηποτε\",\"οποιος\",\"οποιος∆ηποτε\",\"οποιου\",\"οποιους\",\"οποιους∆ηποτε\",\"οποιου∆ηποτε\",\"οποιο∆ηποτε\",\"οποιων\",\"οποιων∆ηποτε\",\"οποι∆ηποτε\",\"οποτε\",\"οποτε∆ηποτε\",\"οπου\",\"οπου∆ηποτε\",\"οπως\",\"οπωσ\",\"ορισμενα\",\"ορισμενες\",\"ορισμενων\",\"ορισμενως\",\"οσα\",\"οσα∆ηποτε\",\"οσες\",\"οσες∆ηποτε\",\"οση\",\"οσην\",\"οσην∆ηποτε\",\"οσης\",\"οσης∆ηποτε\",\"οση∆ηποτε\",\"οσο\",\"οσοι\",\"οσοι∆ηποτε\",\"οσον\",\"οσον∆ηποτε\",\"οσος\",\"οσος∆ηποτε\",\"οσου\",\"οσους\",\"οσους∆ηποτε\",\"οσου∆ηποτε\",\"οσο∆ηποτε\",\"οσων\",\"οσων∆ηποτε\",\"οταν\",\"οτι\",\"οτι∆ηποτε\",\"οτου\",\"ου\",\"ουτε\",\"ου∆ε\",\"οχι\",\"οἱ\",\"οἳ\",\"οἷς\",\"οὐ\",\"οὐδ\",\"οὐδέ\",\"οὐδείσ\",\"οὐδεὶς\",\"οὐδὲ\",\"οὐδὲν\",\"οὐκ\",\"οὐχ\",\"οὐχὶ\",\"οὓς\",\"οὔτε\",\"οὕτω\",\"οὕτως\",\"οὕτωσ\",\"οὖν\",\"οὗ\",\"οὗτος\",\"οὗτοσ\",\"παλι\",\"παντοτε\",\"παντου\",\"παντως\",\"παρ\",\"παρά\",\"παρα\",\"παρὰ\",\"περί\",\"περα\",\"περι\",\"περιπου\",\"περισσοτερο\",\"περσι\",\"περυσι\",\"περὶ\",\"πια\",\"πιθανον\",\"πιο\",\"πισω\",\"πλαι\",\"πλεον\",\"πλην\",\"ποια\",\"ποιαν\",\"ποιας\",\"ποιες\",\"ποιεσ\",\"ποιο\",\"ποιοι\",\"ποιον\",\"ποιος\",\"ποιοσ\",\"ποιου\",\"ποιους\",\"ποιουσ\",\"ποιων\",\"πολυ\",\"ποσες\",\"ποση\",\"ποσην\",\"ποσης\",\"ποσοι\",\"ποσος\",\"ποσους\",\"ποτε\",\"που\",\"πουθε\",\"πουθενα\",\"ποῦ\",\"πρεπει\",\"πριν\",\"προ\",\"προκειμενου\",\"προκειται\",\"προπερσι\",\"προς\",\"προσ\",\"προτου\",\"προχθες\",\"προχτες\",\"πρωτυτερα\",\"πρόσ\",\"πρὸ\",\"πρὸς\",\"πως\",\"πωσ\",\"σαν\",\"σας\",\"σε\",\"σεις\",\"σημερα\",\"σιγα\",\"σου\",\"στα\",\"στη\",\"στην\",\"στης\",\"στις\",\"στο\",\"στον\",\"στου\",\"στους\",\"στων\",\"συγχρονως\",\"συν\",\"συναμα\",\"συνεπως\",\"συνηθως\",\"συχνα\",\"συχνας\",\"συχνες\",\"συχνη\",\"συχνην\",\"συχνης\",\"συχνο\",\"συχνοι\",\"συχνον\",\"συχνος\",\"συχνου\",\"συχνους\",\"συχνων\",\"συχνως\",\"σχε∆ον\",\"σωστα\",\"σόσ\",\"σύ\",\"σύν\",\"σὸς\",\"σὺ\",\"σὺν\",\"τά\",\"τήν\",\"τί\",\"τίς\",\"τίσ\",\"τα\",\"ταυτα\",\"ταυτες\",\"ταυτη\",\"ταυτην\",\"ταυτης\",\"ταυτο,ταυτον\",\"ταυτος\",\"ταυτου\",\"ταυτων\",\"ταχα\",\"ταχατε\",\"ταῖς\",\"τα∆ε\",\"τε\",\"τελικα\",\"τελικως\",\"τες\",\"τετοια\",\"τετοιαν\",\"τετοιας\",\"τετοιες\",\"τετοιο\",\"τετοιοι\",\"τετοιον\",\"τετοιος\",\"τετοιου\",\"τετοιους\",\"τετοιων\",\"τη\",\"την\",\"της\",\"τησ\",\"τι\",\"τινα\",\"τιποτα\",\"τιποτε\",\"τις\",\"τισ\",\"το\",\"τοί\",\"τοι\",\"τοιοῦτος\",\"τοιοῦτοσ\",\"τον\",\"τος\",\"τοσα\",\"τοσες\",\"τοση\",\"τοσην\",\"τοσης\",\"τοσο\",\"τοσοι\",\"τοσον\",\"τοσος\",\"τοσου\",\"τοσους\",\"τοσων\",\"τοτε\",\"του\",\"τουλαχιστο\",\"τουλαχιστον\",\"τους\",\"τουτα\",\"τουτες\",\"τουτη\",\"τουτην\",\"τουτης\",\"τουτο\",\"τουτοι\",\"τουτοις\",\"τουτον\",\"τουτος\",\"τουτου\",\"τουτους\",\"τουτων\",\"τούσ\",\"τοὺς\",\"τοῖς\",\"τοῦ\",\"τυχον\",\"των\",\"τωρα\",\"τό\",\"τόν\",\"τότε\",\"τὰ\",\"τὰς\",\"τὴν\",\"τὸ\",\"τὸν\",\"τῆς\",\"τῆσ\",\"τῇ\",\"τῶν\",\"τῷ\",\"υπ\",\"υπερ\",\"υπο\",\"υποψη\",\"υποψιν\",\"υπό\",\"υστερα\",\"φετος\",\"χαμηλα\",\"χθες\",\"χτες\",\"χωρις\",\"χωριστα\",\"ψηλα\",\"ω\",\"ωραια\",\"ως\",\"ωσ\",\"ωσαν\",\"ωσοτου\",\"ωσπου\",\"ωστε\",\"ωστοσο\",\"ωχ\",\"ἀλλ'\",\"ἀλλά\",\"ἀλλὰ\",\"ἀλλ’\",\"ἀπ\",\"ἀπό\",\"ἀπὸ\",\"ἀφ\",\"ἂν\",\"ἃ\",\"ἄλλος\",\"ἄλλοσ\",\"ἄν\",\"ἄρα\",\"ἅμα\",\"ἐάν\",\"ἐγώ\",\"ἐγὼ\",\"ἐκ\",\"ἐμόσ\",\"ἐμὸς\",\"ἐν\",\"ἐξ\",\"ἐπί\",\"ἐπεὶ\",\"ἐπὶ\",\"ἐστι\",\"ἐφ\",\"ἐὰν\",\"ἑαυτοῦ\",\"ἔτι\",\"ἡ\",\"ἢ\",\"ἣ\",\"ἤ\",\"ἥ\",\"ἧς\",\"ἵνα\",\"ὁ\",\"ὃ\",\"ὃν\",\"ὃς\",\"ὅ\",\"ὅδε\",\"ὅθεν\",\"ὅπερ\",\"ὅς\",\"ὅσ\",\"ὅστις\",\"ὅστισ\",\"ὅτε\",\"ὅτι\",\"ὑμόσ\",\"ὑπ\",\"ὑπέρ\",\"ὑπό\",\"ὑπὲρ\",\"ὑπὸ\",\"ὡς\",\"ὡσ\",\"ὥς\",\"ὥστε\",\"ὦ\",\"ᾧ\",\"∆α\",\"∆ε\",\"∆εινα\",\"∆εν\",\"∆εξια\",\"∆ηθεν\",\"∆ηλα∆η\",\"∆ι\",\"∆ια\",\"∆ιαρκως\",\"∆ικα\",\"∆ικο\",\"∆ικοι\",\"∆ικος\",\"∆ικου\",\"∆ικους\",\"∆ιολου\",\"∆ιπλα\",\"∆ιχως\"]\n",
        "\n",
        "# Remove stopwords\n",
        "def remove_stopwords(text, stopwords_list):\n",
        "    words = text.split()  # Simple tokenization\n",
        "    filtered_words = [word for word in words if word not in stopwords_list]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "train_data['text_without_stopwords'] = train_data['cleaned_text'].apply(lambda x: remove_stopwords(x, greek_stopwords))\n",
        "\n",
        "# Initialize TF-IDF Vectorizer and transform the text data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000, min_df=5, max_df=0.7)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(train_data['text_without_stopwords'])\n",
        "\n",
        "# Splitting the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_tfidf,\n",
        "    train_data['label'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the Support Vector Machine model\n",
        "svm_model = LinearSVC(max_iter=10000)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_val_pred = svm_model.predict(X_val)\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Load and preprocess the test data\n",
        "test_data_path = 'unseen_test_data.csv'\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "test_data['combined_text'] = test_data['title'] + \" \" + test_data['content']\n",
        "test_data['cleaned_text'] = test_data['combined_text'].apply(clean_text)\n",
        "test_data['text_without_stopwords'] = test_data['cleaned_text'].apply(lambda x: remove_stopwords(x, greek_stopwords))\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_data['text_without_stopwords'])\n",
        "\n",
        "# Make predictions on the test dataset\n",
        "y_test_pred = svm_model.predict(X_test_tfidf)\n",
        "test_predictions_df = pd.DataFrame({\n",
        "    'url': test_data['url'],\n",
        "    'content': test_data['content'],\n",
        "    'predicted_label': y_test_pred\n",
        "})\n",
        "\n",
        "# Sample predictions\n",
        "print(test_predictions_df.sample(10))\n",
        "\n"
      ]
    }
  ]
}